from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import LogisticRegressionModel
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType
from pymongo import MongoClient
import pandas as pd
import warnings

warnings.simplefilter(action="ignore",category=FutureWarning)

required_features = ["src_port", "dst_port", "protocol", "bidirectional_packets", "bidirectional_bytes", "src2dst_duration_ms", "src2dst_packets", "src2dst_bytes", "dst2src_duration_ms", "dst2src_packets", "dst2src_bytes", "bidirectional_mean_ps", "bidirectional_stddev_ps", "bidirectional_max_ps", "src2dst_min_ps", "src2dst_mean_ps", "src2dst_stddev_ps", "src2dst_max_ps", "dst2src_min_ps", "dst2src_mean_ps", "dst2src_stddev_ps", "dst2src_max_ps", "bidirectional_min_piat_ms", "bidirectional_mean_piat_ms", "bidirectional_stddev_piat_ms", "bidirectional_max_piat_ms", "src2dst_min_piat_ms", "src2dst_mean_piat_ms", "src2dst_stddev_piat_ms", "src2dst_max_piat_ms", "dst2src_min_piat_ms", "dst2src_mean_piat_ms", "dst2src_stddev_piat_ms", "dst2src_max_piat_ms", "bidirectional_syn_packets", "bidirectional_cwr_packets", "bidirectional_ece_packets", "bidirectional_urg_packets", "bidirectional_ack_packets", "bidirectional_psh_packets", "bidirectional_rst_packets", "bidirectional_fin_packets", "src2dst_syn_packets", "src2dst_cwr_packets", "src2dst_ece_packets", "src2dst_urg_packets", "src2dst_ack_packets", "src2dst_psh_packets", "src2dst_rst_packets", "src2dst_fin_packets", "dst2src_syn_packets", "dst2src_cwr_packets", "dst2src_ece_packets", "dst2src_urg_packets", "dst2src_ack_packets", "dst2src_psh_packets", "dst2src_rst_packets", "dst2src_fin_packets","label"]


def build_model(spark, data_set_path):
    
	df = spark.read.format("csv").option('header','true').load(data_set_path) # reading the nfstream data from the csv file
	df = df.fillna(0) # filling the null values with 0
	for ii in required_features: # converting the data type of the columns to integer
			df=df.withColumn(ii,col(ii).cast(IntegerType()))
	features = required_features[:-1]
	va = VectorAssembler(inputCols = features, outputCol='features') # converting the features into vector
	va_df = va.transform(df) # transforming the data
	va_df = va_df.select(['features', 'label']) 
	lr = LogisticRegression(labelCol="label",maxIter=10)
	model=lr.fit(va_df) 
	model.save("logisticregression_classifer1.model") # saving the model
	
    
def write_data_to_mongodb(spark) -> None:
    """Collects the data from kafka and writes it to mongodb
    Kafka:
    topic: project, server: localhost:9092
    Kafka Data: 
    key : device_id
    value : time, features
    """
    topic = "project"
    kafka_server_ip = "localhost"
    kafka_server_port = "9092"
    df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", f"{kafka_server_ip}:{kafka_server_port}").option("subscribe", topic).load()
    
    
    def insert_df(kafka_data): # function to insert the data into mongodb [callback function for 'foreach']
        device_id = kafka_data.key.decode() 
        kafka_data = kafka_data.value.decode().split(",")
        (time, features) = kafka_data[0], kafka_data[1:]
        features = [int(x) if "." not in x else float(x) for x in features] # converting the strings in to int, float

        mongo_client = MongoClient("mongodb://localhost:27017/")
        mongo_db = mongo_client["bda_project"]
        db_collection = mongo_db["raw_data"] # creating a mongodb collection object

        _ = db_collection.insert_one({"device_name":device_id,"raw_data": features}) # inserting the data into the mongodb database
        
    df.writeStream.foreach(insert_df).start()

def classify_data(spark) -> None:
    """classifing the data using the model and updating to mongodb database\n"""
    
    lr_model = LogisticRegressionModel.load("logisticregression_classifer1.model")
    
    mongo_client = MongoClient("mongodb://localhost:27017/")
    mongo_db = mongo_client["bda_project"]
    db_collection = mongo_db["raw_data"] # creating a mongodb collection object
    
    va = VectorAssembler(inputCols = required_features[:-1], outputCol='features') 
    while True:
        data = db_collection.find({"remark":{"$exists":False}}).limit(100) # fetching 100 collections from mongodb
        for ii in data:
            device_id = ii["device_name"]
            features = ii["raw_data"]

            pdf = pd.DataFrame([features], columns=required_features[:-1])
            # Converting the Pandas DataFrame to a Spark DataFrame
            test_df = spark.createDataFrame(pdf)

            data_pred = va.transform(test_df)

            data_pred = lr_model.transform(data_pred) # classification
            remark = data_pred.select("prediction").first()[0] 

            _ = db_collection.update_one({"_id":ii["_id"]},{"$set":{"remark":int(remark)}}) # updating into mongodb
            

